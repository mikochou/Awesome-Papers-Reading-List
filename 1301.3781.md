# 1301.3781 Efficient Estimation of Word Representations in Vector Space
 [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf) 

* 创新点 

log-linear模型，去掉神经网络的隐藏层 
* 方法 

前面两种模型： 
Feedforward Neural Net Language Model (NNLM)： 
input, projection, hidden and output layers 
Recurrent Neural Net Language Model (RNNLM）： 
input, hidden and output layer 
the recurrent matrix that connects hidden layer to itself, using time-delayed connections 

New Log-linear Models： 
The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model 
first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words 
去除隐藏层,将词语实数向量的计算和神经网络对Ngram的训练相分开
 
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g6tmddnk6sj31do0u00zb.jpg)

总之就是一个是根据上下文预测当前词语，另一个根据当前词语预测上下文的意思8 
* 数据集 & 实验 
Google News corpus: 6B tokens, the most frequent 30k words 
NNLM和Skip-gram表现的比较好吧，不同的epoch没有什么太大的差别 

